{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e635f0db-cabc-4c6e-b611-ffd571037b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SparkConf** sets the configuration for a Spark application & \n",
    "**SparkContext** initializes the connection to the Spark cluster using that configuration\n",
    "below code will throw an error cause in Databrics we use **SparkSession** instead of manually creating SparkConf and SparkContext, since Databricks handles those under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7385e67d-447d-44f2-9eb5-99505c3d2791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2016912667789946>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext , SparkConf\n",
       "\u001B[1;32m      2\u001B[0m conf \u001B[38;5;241m=\u001B[39m SparkConf()\u001B[38;5;241m.\u001B[39msetAppName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSparkAppName\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msetMaster(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocal[*]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m----> 3\u001B[0m sc \u001B[38;5;241m=\u001B[39m SparkContext(conf\u001B[38;5;241m=\u001B[39mconf)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n",
       "\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    200\u001B[0m     )\n",
       "\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n",
       "\u001B[1;32m    205\u001B[0m         master,\n",
       "\u001B[1;32m    206\u001B[0m         appName,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n",
       "\u001B[1;32m    217\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n",
       "\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n",
       "\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n",
       "\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n",
       "\u001B[1;32m    493\u001B[0m             currentAppName,\n",
       "\u001B[1;32m    494\u001B[0m             currentMaster,\n",
       "\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n",
       "\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n",
       "\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n",
       "\u001B[1;32m    498\u001B[0m         )\n",
       "\u001B[1;32m    499\u001B[0m     )\n",
       "\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-2016912667789946>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext , SparkConf\n\u001B[1;32m      2\u001B[0m conf \u001B[38;5;241m=\u001B[39m SparkConf()\u001B[38;5;241m.\u001B[39msetAppName(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSparkAppName\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msetMaster(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocal[*]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m sc \u001B[38;5;241m=\u001B[39m SparkContext(conf\u001B[38;5;241m=\u001B[39mconf)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    200\u001B[0m     )\n\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[1;32m    205\u001B[0m         master,\n\u001B[1;32m    206\u001B[0m         appName,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n\u001B[1;32m    217\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n\u001B[1;32m    493\u001B[0m             currentAppName,\n\u001B[1;32m    494\u001B[0m             currentMaster,\n\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n\u001B[1;32m    498\u001B[0m         )\n\u001B[1;32m    499\u001B[0m     )\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyspark import SparkContext , SparkConf\n",
    "# conf = SparkConf().setAppName(\"SparkAppName\").setMaster(\"local[*]\")\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfa28a08-824e-4356-ba23-4071f32795d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SparkSession.builder** : Entry point to programming with Spark using Dataframe & SQL API. <br>\n",
    "**builder** : builder is builder pattern to configure & create sparksession object. <br>\n",
    "**appName** : It's Name of Spark Application. <br>\n",
    "**getOrCreate** : returns an existing SparkSession, if not then it creates new one. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea5fb0d-51a0-46a8-bd59-0c9b0421addb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkAppName\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2999ea4-fc72-41f6-af4e-2bb27348f0a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below code will return spark configuration information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ce59a2-d432-4c65-a4c6-751bd1f0efbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.databricks.preemption.enabled = true\nspark.sql.hive.metastore.jars = /databricks/databricks-hive/*\nspark.driver.tempDirectory = /local_disk0/tmp\nspark.sql.warehouse.dir = dbfs:/user/hive/warehouse\nspark.databricks.managedCatalog.clientClassName = com.databricks.managedcatalog.ManagedCatalogClientImpl\nspark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName = com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider\nspark.hadoop.fs.fcfs-s3.impl.disable.cache = true\nspark.sql.streaming.checkpointFileManagerClass = com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager\nspark.databricks.service.dbutils.repl.backend = com.databricks.dbconnect.ReplDBUtils\nspark.hadoop.databricks.s3.verifyBucketExists.enabled = false\nspark.streaming.driver.writeAheadLog.allowBatching = true\nspark.databricks.clusterSource = UI\nspark.hadoop.hive.server2.transport.mode = http\nspark.databricks.acl.dfAclsEnabled = false\nspark.databricks.clusterUsageTags.orgId = 2258649537226512\nspark.executor.memory = 8278m\nspark.hadoop.spark.driverproxy.customHeadersToProperties = X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name,X-Databricks-Oauth-Identity-Custom-Claim:spark.databricks.oauthCustomIdentityClaims,X-Databricks-Workload-Id:spark.databricks.workload.id,X-Databricks-Workload-Class:spark.databricks.workload.name\nspark.databricks.clusterUsageTags.driverContainerId = a458030f80ba48ed9036bb56502669ac\nspark.hadoop.fs.cpfs-adl.impl.disable.cache = true\nspark.databricks.clusterUsageTags.hailEnabled = false\nspark.databricks.clusterUsageTags.clusterLogDeliveryEnabled = false\nspark.databricks.clusterUsageTags.containerType = LXC\nspark.hadoop.fs.s3a.assumed.role.credentials.provider = shaded.databricks.org.apache.hadoop.fs.s3a.DatabricksInstanceProfileCredentialsProvider\nspark.eventLog.enabled = false\nspark.hadoop.fs.stage.impl.disable.cache = true\nspark.hadoop.hive.hmshandler.retry.interval = 2000\nspark.executor.tempDirectory = /local_disk0/tmp\nspark.hadoop.fs.azure.authorization.caching.enable = false\nspark.hadoop.fs.fcfs-abfss.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.hadoop.mapred.output.committer.class = com.databricks.backend.daemon.data.client.DirectOutputCommitter\nspark.hadoop.hive.server2.thrift.http.port = 10000\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version = 2\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3 = 0\nspark.sql.allowMultipleContexts = false\nspark.databricks.eventLog.enabled = true\nspark.databricks.sparkContextId = 4272312072083009428\nspark.home = /databricks/spark\nspark.databricks.clusterUsageTags.clusterTargetWorkers = 0\nspark.hadoop.hive.server2.idle.operation.timeout = 7200000\nspark.task.reaper.enabled = true\nspark.databricks.clusterUsageTags.autoTerminationMinutes = 60\nspark.storage.memoryFraction = 0.5\nspark.databricks.clusterUsageTags.clusterFirstOnDemand = 0\nspark.databricks.sql.configMapperClass = com.databricks.dbsql.config.SqlConfigMapperBridge\nspark.driver.maxResultSize = 4g\nspark.databricks.clusterUsageTags.sparkEnvVarContainsNewline = false\nspark.hadoop.fs.fcfs-s3.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.delta.multiClusterWrites.enabled = true\nspark.worker.cleanup.enabled = false\nspark.sql.legacy.createHiveTableByDefault = false\nspark.databricks.driver.preferredMavenCentralMirrorUrl = https://maven-central.storage-download.googleapis.com/maven2/\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File = 0\nspark.hadoop.fs.fcfs-s3a.impl.disable.cache = true\nspark.ui.port = 40001\nspark.hadoop.fs.s3a.attempts.maximum = 10\nspark.databricks.clusterUsageTags.enableCredentialPassthrough = false\nspark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign = false\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeType = ebs_volume_type: GENERAL_PURPOSE_SSD\n\nspark.databricks.clusterUsageTags.enableJdbcAutoStart = true\nspark.hadoop.fs.azure.user.agent.prefix = \nspark.hadoop.fs.s3n.impl.disable.cache = true\nspark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough = false\nspark.hadoop.fs.fcfs-s3n.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.hadoop.fs.abfs.impl = com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.s3a.retry.throttle.interval = 500ms\nspark.hadoop.fs.wasb.impl.disable.cache = true\nspark.databricks.clusterUsageTags.clusterLogDestination = \nspark.databricks.wsfsPublicPreview = true\nspark.app.startTime = 1747984143083\nspark.cleaner.referenceTracking.blocking = false\nspark.databricks.clusterUsageTags.clusterState = Pending\nspark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes = false\nspark.databricks.tahoe.logStore.azure.class = com.databricks.tahoe.store.AzureLogStore\nspark.databricks.clusterUsageTags.driverInstanceId = i-0a53b6ead3762156a\nspark.hadoop.fs.azure.skip.metrics = true\nspark.hadoop.hive.hmshandler.retry.attempts = 10\nspark.scheduler.mode = FAIR\nspark.databricks.clusterUsageTags.driverPublicDns = ec2-34-214-215-107.us-west-2.compute.amazonaws.com\nspark.sql.sources.default = delta\nspark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled = true\nspark.databricks.clusterUsageTags.clusterOwnerOrgId = 2258649537226512\nspark.databricks.clusterUsageTags.clusterWorkers = 0\nspark.hadoop.fs.cpfs-s3n.impl = com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.hadoop.fs.cpfs-adl.impl = com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.hadoop.fs.fcfs-s3n.impl.disable.cache = true\nspark.hadoop.fs.cpfs-abfss.impl = com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.databricks.rocksDB.fileManager.useCommitService = false\nspark.databricks.passthrough.oauth.refresher.impl = com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient\nspark.hadoop.databricks.loki.fileStatusCache.gcs.enabled = false\nspark.sql.hive.metastore.sharedPrefixes = org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\nspark.databricks.io.directoryCommit.enableLogicalDelete = false\nspark.task.reaper.killTimeout = 60s\nspark.databricks.clusterUsageTags.attribute_tag_dust_runner = \nspark.hadoop.parquet.block.size.row.check.min = 10\nspark.hadoop.hive.server2.use.SSL = true\nspark.databricks.clusterUsageTags.clusterAvailability = ON_DEMAND\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb = 0\nspark.databricks.clusterUsageTags.attribute_tag_platform_name = \nspark.databricks.clusterUsageTags.clusterLogDestinationType = \nspark.hadoop.hive.server2.keystore.path = /databricks/keys/jetty-ssl-driver-keystore.jks\nspark.databricks.deltaSharing.clientClassName = com.databricks.deltasharing.DataSharingClientImpl\nspark.hadoop.fs.gs.impl = com.databricks.common.filesystem.LokiFileSystem\nspark.driver.port = 45661\nspark.databricks.credential.redactor = com.databricks.logging.secrets.CredentialRedactorProxyImpl\nspark.databricks.clusterUsageTags.clusterPinned = false\nspark.databricks.acl.provider = com.databricks.sql.acl.ReflectionBackedAclProvider\nspark.databricks.wsfs.workspacePrivatePreview = true\nspark.databricks.mlflow.autologging.enabled = true\nspark.extraListeners = com.databricks.backend.daemon.driver.DBCEventLoggingListener\nspark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled = false\nspark.sql.parquet.cacheMetadata = true\nspark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2 = 0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss = 0\nspark.hadoop.parquet.abfs.readahead.optimization.enabled = true\nspark.hadoop.fs.adl.impl = com.databricks.adl.AdlFileSystem\nspark.hadoop.fs.cpfs-abfss.impl.disable.cache = true\nspark.hadoop.fs.abfss.impl = com.databricks.common.filesystem.LokiFileSystem\nspark.databricks.clusterUsageTags.enableLocalDiskEncryption = false\nspark.databricks.tahoe.logStore.class = com.databricks.tahoe.store.DelegatingLogStore\nspark.hadoop.fs.s3.impl.disable.cache = true\nspark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins = 30\nspark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins = 30\nlibraryDownload.sleepIntervalSeconds = 5\nspark.databricks.cloudProvider = AWS\nspark.sql.hive.convertMetastoreParquet = true\nspark.executor.id = driver\nspark.databricks.service.dbutils.server.backend = com.databricks.dbconnect.SparkServerDBUtils\nspark.databricks.clusterUsageTags.workerEnvironmentId = default-worker-env\nspark.repl.class.outputDir = /local_disk0/tmp/repl/spark-4272312072083009428-1252b39b-9ece-46ef-a402-e05ae3d4faad\nspark.databricks.clusterUsageTags.clusterAllTags = [{\"key\":\"Name\",\"value\":\"ce-worker\"},{\"key\":\"WorkspaceId\",\"value\":\"2258649537226512\"},{\"key\":\"ClusterId\",\"value\":\"0523-070816-m6e0pgk5\"}]\nspark.databricks.repl.enableClassFileCleanup = true\nspark.databricks.clusterUsageTags.clusterName = My Cluster\nspark.hadoop.fs.s3a.multipart.size = 10485760\nspark.databricks.clusterUsageTags.cloudProvider = AWS\nspark.metrics.conf = /databricks/spark/conf/metrics.properties\nspark.akka.frameSize = 256\nspark.hadoop.fs.s3a.fast.upload = true\nspark.repl.class.uri = spark://10.172.234.106:45661/classes\nspark.hadoop.fs.wasbs.impl = shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\nspark.sql.streaming.stopTimeout = 15s\nspark.hadoop.hive.server2.keystore.password = [REDACTED]\nspark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting = false\nspark.hadoop.fs.s3a.retry.interval = 250ms\nspark.databricks.clusterUsageTags.sparkEnvVarContainsEscape = false\nspark.databricks.overrideDefaultCommitProtocol = org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\nspark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass = com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient\nspark.databricks.clusterUsageTags.clusterNoDriverDaemon = false\nlibraryDownload.timeoutSeconds = 180\nspark.hadoop.parquet.memory.pool.ratio = 0.5\nspark.databricks.clusterUsageTags.shardName = devtierprod1\nspark.databricks.clusterUsageTags.clusterScalingType = fixed_size\nspark.hadoop.databricks.loki.fileStatusCache.abfs.enabled = false\nspark.databricks.passthrough.adls.gen2.tokenProviderClassName = com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider\nspark.hadoop.fs.s3a.block.size = 67108864\nspark.databricks.tahoe.logStore.gcp.class = com.databricks.tahoe.store.GCPLogStore\nspark.serializer.objectStreamReset = 100\nspark.databricks.clusterUsageTags.sparkMasterUrlType = None\nspark.databricks.passthrough.enabled = false\nspark.sql.sources.commitProtocolClass = com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs = 0\nspark.databricks.clusterUsageTags.effectiveSparkVersion = 12.2.x-scala2.12\nspark.databricks.clusterUsageTags.attribute_tag_budget = \nspark.hadoop.fs.fcfs-s3a.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.clusterUsageTags.currentAttemptContainerZoneId = us-west-2c\nspark.databricks.clusterUsageTags.clusterPythonVersion = 3\nspark.databricks.clusterUsageTags.enableDfAcls = false\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount = 0\nspark.hadoop.databricks.loki.fileSystemCache.enabled = true\nspark.shuffle.service.enabled = true\nspark.hadoop.fs.file.impl = com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem\nspark.plugins = org.apache.spark.sql.connect.SparkConnectPlugin\nspark.hadoop.fs.fcfs-wasb.impl.disable.cache = true\nspark.hadoop.fs.cpfs-s3.impl = com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.databricks.clusterUsageTags.containerZoneId = auto\nspark.databricks.clusterUsageTags.attribute_tag_dust_maintainer = \nspark.hadoop.fs.s3a.multipart.threshold = 104857600\nspark.rpc.message.maxSize = 256\nspark.databricks.clusterUsageTags.attribute_tag_dust_suite = \nspark.hadoop.fs.fcfs-wasbs.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.driverNfs.enabled = true\nspark.databricks.clusterUsageTags.clusterMetastoreAccessType = RDS_DIRECT\nspark.databricks.clusterUsageTags.ngrokNpipEnabled = false\nspark.hadoop.parquet.page.metadata.validation.enabled = true\nspark.databricks.acl.enabled = false\nspark.databricks.clusterUsageTags.instanceProfileUsed = false\nspark.app.id = local-1747984150534\nspark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName = com.databricks.unity.TokenServiceApiTokenProvider\nspark.databricks.passthrough.glue.executorServiceFactoryClassName = com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory\nspark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus = false\nspark.databricks.clusterUsageTags.attribute_tag_dust_resource_class = \nspark.databricks.acl.scim.client = com.databricks.spark.sql.acl.client.DriverToWebappScimClient\nspark.r.sql.derby.temp.dir = /tmp/Rtmpct67Tg\nspark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick = false\nspark.databricks.clusterUsageTags.isSingleUserCluster = true\nspark.hadoop.fs.adl.impl.disable.cache = true\nspark.hadoop.parquet.block.size.row.check.max = 10\nspark.hadoop.fs.s3a.connection.maximum = 200\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2 = 0\nspark.hadoop.fs.s3a.fast.upload.active.blocks = 32\nspark.shuffle.reduceLocality.enabled = false\nspark.databricks.clusterUsageTags.driverNodeType = dev-tier-node\nspark.hadoop.spark.sql.sources.outputCommitterClass = com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter\nspark.hadoop.fs.fcfs-abfs.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.clusterUsageTags.instanceBootstrapType = ssh\nspark.hadoop.fs.fcfs-abfss.impl.disable.cache = true\nspark.hadoop.hive.server2.thrift.http.cookie.auth.enabled = false\nspark.hadoop.spark.hadoop.aws.glue.cache.table.size = 1000\nspark.databricks.driverNodeTypeId = dev-tier-node\nspark.sql.parquet.compression.codec = snappy\nspark.hadoop.fs.stage.impl = com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem\nspark.databricks.credential.scope.fs.s3a.tokenProviderClassName = com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider\nspark.databricks.cloudfetch.hasRegionSupport = true\nspark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories = false\nspark.hadoop.fs.wasb.impl = shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem\nspark.hadoop.spark.hadoop.aws.glue.cache.db.size = 1000\nspark.databricks.unityCatalog.enabled = false\nspark.databricks.workerNodeTypeId = dev-tier-node\nspark.databricks.passthrough.glue.credentialsProviderFactoryClassName = com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory\nspark.databricks.clusterUsageTags.clusterEbsVolumeSize = 0\nspark.sparklyr-backend.threads = 1\nspark.hadoop.fs.fcfs-wasb.impl = com.databricks.sql.acl.fs.FixedCredentialsFileSystem\nspark.databricks.passthrough.s3a.tokenProviderClassName = com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider\nspark.databricks.session.share = false\nspark.databricks.clusterUsageTags.clusterResourceClass = default\nspark.databricks.isShieldWorkspace = false\nspark.hadoop.fs.idbfs.impl = com.databricks.io.idbfs.IdbfsFileSystem\nspark.driver.extraJavaOptions = -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\nspark.databricks.cloudfetch.requestDownloadUrlsWithHeaders = false\nspark.databricks.telemetry.prometheus.samplingRate = 100\nspark.databricks.clusterUsageTags.sparkImageLabel = release__12.2.x-snapshot-scala2.12__databricks__12.2.48__34641ba__c952064__jenkins__46777fa__format-3\nspark.hadoop.fs.dbfs.impl = com.databricks.backend.daemon.data.client.DbfsHadoop3\nspark.databricks.clusterUsageTags.clusterSku = STANDARD_SKU\nspark.hadoop.fs.gs.impl.disable.cache = true\nspark.databricks.privateLinkEnabled = false\nspark.delta.sharing.profile.provider.class = io.delta.sharing.DeltaSharingCredentialsProvider\nspark.executor.extraJavaOptions = -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1\nspark.databricks.clusterUsageTags.clusterId = 0523-070816-m6e0pgk5\nspark.databricks.clusterUsageTags.isGroupCluster = false\nspark.worker.aioaLazyConfig.iamReadinessCheckClientClass = com.databricks.backend.daemon.driver.NephosIamRoleCheckClient\nspark.databricks.clusterUsageTags.clusterEbsVolumeType = GENERAL_PURPOSE_SSD\nspark.databricks.automl.serviceEnabled = true\nspark.hadoop.parquet.page.size.check.estimate = false\nspark.databricks.clusterUsageTags.attribute_tag_service = \nspark.databricks.passthrough.s3a.threadPoolExecutor.factory.class = com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory\nspark.databricks.metrics.filesystem_io_metrics = true\nspark.databricks.cloudfetch.requesterClassName = com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester\nspark.master = local[8]\nspark.databricks.delta.logStore.crossCloud.fatal = true\nspark.databricks.driverNfs.clusterWidePythonLibsEnabled = true\nspark.driver.host = 10.172.234.106\nspark.files.fetchFailure.unRegisterOutputOnHost = true\nspark.databricks.driver.enableDncOomMessage = false\nspark.databricks.clusterUsageTags.enableSqlAclsOnly = false\nspark.databricks.clusterUsageTags.clusterEbsVolumeCount = 0\nspark.databricks.clusterUsageTags.clusterSizeType = VM_CONTAINER\nspark.hadoop.databricks.fs.perfMetrics.enable = true\nspark.databricks.clusterUsageTags.clusterNumSshKeys = 0\nspark.hadoop.fs.gs.outputstream.upload.chunk.size = 16777216\nspark.databricks.tahoe.logStore.aws.class = com.databricks.tahoe.store.S3LockBasedLogStore\nspark.speculation.quantile = 0.9\nspark.databricks.clusterUsageTags.privateLinkEnabled = false\nspark.shuffle.manager = SORT\nspark.files.overwrite = true\nspark.databricks.credential.aws.secretKey.redactor = com.databricks.spark.util.AWSSecretKeyRedactorProxy\nspark.databricks.clusterUsageTags.clusterNumCustomTags = 0\nspark.hadoop.fs.s3.impl = com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.s3a.impl.disable.cache = true\nspark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes = false\nspark.r.numRBackendThreads = 1\nspark.hadoop.fs.wasbs.impl.disable.cache = true\nspark.hadoop.fs.abfss.impl.disable.cache = true\nspark.hadoop.fs.azure.cache.invalidator.type = com.databricks.encryption.utils.CacheInvalidatorImpl\nspark.sql.hive.metastore.version = 0.13.0\nspark.shuffle.service.port = 4048\nspark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType = default\nspark.databricks.acl.client = com.databricks.spark.sql.acl.client.SparkSqlAclClient\nspark.streaming.driver.writeAheadLog.closeFileAfterWrite = true\nspark.hadoop.hive.warehouse.subdir.inherit.perms = false\nspark.databricks.clusterUsageTags.attribute_tag_dust_runbot_id = \nspark.databricks.clusterUsageTags.clusterNodeTypeFlexibilityEnabled = false\nspark.databricks.clusterUsageTags.runtimeEngine = STANDARD\nspark.databricks.safer.unifiedPath.applyFlag.enabled = false\nspark.databricks.clusterUsageTags.isServicePrincipalCluster = false\nspark.databricks.credential.scope.fs.impl = com.databricks.sql.acl.fs.CredentialScopeFileSystem\nspark.hadoop.databricks.loki.fileStatusCache.s3a.enabled = false\nspark.databricks.clusterUsageTags.userId = 2633380832704282\nspark.databricks.enablePublicDbfsFuse = false\nspark.databricks.clusterUsageTags.enableElasticDisk = false\nspark.hadoop.fs.fcfs-wasbs.impl.disable.cache = true\nspark.databricks.clusterUsageTags.userProvidedSparkVersion = 12.2.x-scala2.12\nspark.databricks.clusterUsageTags.clusterNodeType = dev-tier-node\nspark.databricks.passthrough.adls.tokenProviderClassName = com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider\nspark.app.name = Databricks Shell\nspark.driver.allowMultipleContexts = false\nspark.hadoop.fs.AbstractFileSystem.gs.impl = shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\nspark.databricks.secret.sparkConf.keys.toRedact = \nspark.rdd.compress = true\nspark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException = false\nspark.databricks.python.defaultPythonRepl = ipykernel\nspark.hadoop.fs.s3a.retry.limit = 6\nspark.databricks.clusterUsageTags.attribute_tag_dust_execution_env = \nspark.databricks.clusterUsageTags.isIMv2Enabled = true\nspark.databricks.eventLog.dir = eventlogs\nspark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled = false\nspark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName = com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider\nspark.databricks.driverNfs.pathSuffix = .ephemeral_nfs\nspark.databricks.clusterUsageTags.clusterCreator = Webapp\nspark.speculation = false\nspark.hadoop.databricks.dbfs.client.version = v1\nspark.hadoop.hive.server2.session.check.interval = 60000\nspark.sql.hive.convertCTAS = true\nspark.hadoop.fs.s3a.max.total.tasks = 1000\nspark.hadoop.spark.sql.parquet.output.committer.class = org.apache.spark.sql.parquet.DirectParquetOutputCommitter\nspark.databricks.clusterUsageTags.sparkVersion = 12.2.x-scala2.12\nspark.hadoop.fs.s3a.fast.upload.default = true\nspark.databricks.clusterUsageTags.clusterGeneration = 0\nspark.hadoop.fs.mlflowdbfs.impl = com.databricks.mlflowdbfs.MlflowdbfsFileSystem\nspark.databricks.clusterUsageTags.clusterUnityCatalogMode = LEGACY_SINGLE_USER_STANDARD\nspark.databricks.eventLog.listenerClassName = com.databricks.backend.daemon.driver.DBCEventLoggingListener\nspark.hadoop.fs.abfs.impl.disable.cache = true\nspark.speculation.multiplier = 3\nspark.storage.blockManagerTimeoutIntervalMs = 300000\nspark.databricks.clusterUsageTags.clusterOwnerUserId = 2633380832704282\nspark.databricks.clusterUsageTags.instanceWorkerEnvId = default-worker-env\nspark.databricks.clusterUsageTags.driverContainerPrivateIp = 10.172.234.106\nspark.sparkr.use.daemon = false\nspark.scheduler.listenerbus.eventqueue.capacity = 20000\nspark.hadoop.fs.s3a.impl = com.databricks.common.filesystem.LokiFileSystem\nspark.databricks.clusterUsageTags.clusterStateMessage = Starting Spark\nspark.hadoop.parquet.page.write-checksum.enabled = true\nspark.hadoop.databricks.s3commit.client.sslTrustAll = false\nspark.hadoop.fs.s3a.threads.max = 136\nspark.r.backendConnectionTimeout = 604800\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs = 0\nspark.hadoop.fs.s3n.impl = com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.hive.server2.idle.session.timeout = 900000\nspark.databricks.redactor = com.databricks.spark.util.DatabricksSparkLogRedactorProxy\nspark.executor.extraClassPath = /databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*\nspark.databricks.autotune.maintenance.client.classname = com.databricks.maintenanceautocompute.MACClientImpl\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes = 0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace = 0\nspark.hadoop.fs.fcfs-abfs.impl.disable.cache = true\nspark.hadoop.parquet.page.verify-checksum.enabled = true\nspark.databricks.clusterUsageTags.dataPlaneRegion = us-west-2\nspark.logConf = true\nspark.databricks.clusterUsageTags.driverInstancePrivateIp = 10.172.230.61\nspark.databricks.clusterUsageTags.enableJobsAutostart = true\nspark.hadoop.hive.server2.enable.doAs = false\nspark.hadoop.parquet.filter.columnindex.enabled = false\nspark.shuffle.memoryFraction = 0.2\nspark.hadoop.fs.dbfsartifacts.impl = com.databricks.backend.daemon.data.client.DBFSV1\nspark.hadoop.fs.cpfs-s3a.impl = com.databricks.sql.acl.fs.CredentialPassthroughFileSystem\nspark.databricks.clusterUsageTags.attribute_tag_dust_bazel_path = \nspark.hadoop.fs.s3a.connection.timeout = 50000\nspark.databricks.secret.envVar.keys.toRedact = \nspark.databricks.clusterUsageTags.region = us-west-2\nspark.databricks.clusterUsageTags.clusterSpotBidPricePercent = 100\nspark.files.useFetchCache = false\n"
     ]
    }
   ],
   "source": [
    "for item in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{item[0]} = {item[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be119d1-e1c8-468d-9122-13b539ce6aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: <bound method SparkContext.parallelize of <SparkContext master=local[8] appName=Databricks Shell>>"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.parallelize ## returns total number of cores allocated to Spark application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be924659-86a9-48ef-908b-61f8dab4cc76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating a simple dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088c6abf-e676-415c-a807-03f50a69074f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| id| name|\n+---+-----+\n|  1|Alice|\n|  2|  Bob|\n+---+-----+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>Alice</td></tr><tr><td>2</td><td>Bob</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice"
        ],
        [
         2,
         "Bob"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\")], [\"id\", \"name\"])\n",
    "df.show() ## show top 20 rows\n",
    "display(df) ## shows entire dataset in structured manner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1085ee06-6a2c-4ac5-bfef-72ddb2603e97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplceTempView(\"SQLTable\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. Configure Pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}